---
title : "간과하기 쉬운 딥러닝 개념 요소들"
tag : 
 - 퍼셉트론, Perceptron
 - 히든레이어, Hidden layer
 - 히든사이즈, Hidden size
---

## What is the perceptron? hidden layer? ##

이번 포스팅에서는 퍼셉트론과 히든레이어 개념에 대해 정확히 설명하겠습니다. 딥 러닝 기초를 공부하시면서 필수적으로 보는 내용인데요, 기초를 다루는 내용이라 그런지 보통 추상적으로 이해하고 넘어가는 경우가 많은 것 같습니다.

이후에 좀 더 심도있는 공부를 하다 보면, 히든레이어? 수 많은 노드들로 구성된 중간에 있는 레이어 아냐?? 퍼셉트론? 노드들인가?? 정확한 정의가 뭐지?? 하며, 기본 근간 지식이 흔들리는 때가 옵니다. 저 역시 마찬가지로 겪은 문제라 이번 기회에 정확히 짚고 넘어가고자 합니다.

퍼셉트론(Perceptron)이란?

딥러닝은 기본적으로 입력값(Features)에 가중치(Weight)를 곱한 후 Bias를 더하여 출력값 y를 만들어 냅니다. y1 = w1x1 + w2x2 + .... + b

이 전체의 구조=수식을 퍼셉트론이라 합니다.

아래 그림을 보시면, 총 2개의 퍼셉트론이 병렬로 존재하는 것을 확인할 수 있습니다. y1에 묶여 있는 수식 하나, 그리고 y2에 묶여 있는 수식 둘.

![image](/assets/img/2020-12-29_perceptron.jpg)
> 그림들은 봉수골 개발자 이선비(Youtube)님의 자료를 가져왔습니다.

퍼셉트론에 이어지는 개념인 히든 레이어에 대해 이어 설명드리겠습니다.

히든 레이어(Hidden Layer)란?

입력값(Features)를 구성하는 Input Layer와 출력값(Output)으로 구성되는 Output Layer 사이에 퍼셉트론(수식)을 추가하여 하나의 별도 Layer로 구성한 것을 히든 레이어라고 합니다.

아래 그림을 보시면 수월하게 이해할 수 있습니다.

![image](/assets/img/2020-12-29_perceptron.jpg)

파란색 층인 Input Layer, 빨간색 층인 Hidden Layer, 초록색 층인 Output Layer.

진한 파랑 선으로 만들어진 삼각형은 1개의 퍼셉트론을 의미합니다. h1 = x1w1 + x2w2 +... + b (위 그림에서는 bias는 표현상으로만 생략)

h1 ~ h5까지 총 5개의 노드가 있으니 인풋 레이어와 히든 레이어 사이에는 총 5개의 퍼셉트론이 있는 것을 알 수 있습니다.

그리고 진한 빨강 선으로 만들어진 사각형 역시 퍼셉트론 입니다. 히든 레이어가 가지는 모든 노드들을 입력으로 하고, 웨이트 간선을 거쳐 출력값 y에 연결되어 있습니다.

즉, y = h1w1 + h2w2 + h3w3 + h4w4 + h5w5 + bias(표현상으로만 bias 생략), 1개의 퍼셉트론이 있습니다.

정리하자면, 위 그림은 Input Layer와 Hidden Layer 사이에 5개의 퍼셉트론. Hidden Layer와 Output Layer 사이에는 1개의 퍼셉트론으로 구성된 신경망 모델이라고 정의할 수 있습니다.